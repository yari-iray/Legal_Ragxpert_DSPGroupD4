{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge graph example notebook\n",
    "#### This notebook is intended for easier development of our chatbot and is mostly intended as a stop-gap solution to validate the rag components before implementing them into the front end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'required packages:\\n- langchain\\n- langchain-ollama\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from setting import RAGSettings\n",
    "from langchain_community.chains.graph_qa.base import GraphQAChain\n",
    "from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import (\n",
    "    create_history_aware_retriever,\n",
    "    create_retrieval_chain,\n",
    ")\n",
    "\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain_neo4j import GraphCypherQAChain, Neo4jGraph\n",
    "\n",
    "\"\"\"required packages:\n",
    "- langchain\n",
    "- langchain-ollama\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_components_are_running = True\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT_RAG_EN = \"\"\"From now on you are an expert on movies. \n",
    "The way you garner your knowledge is by generating Cyper query language statements to query a database.\n",
    "This should be your only ouput\"\"\"\n",
    "\n",
    "setting = RAGSettings()\n",
    "host = \"localhost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ollama\n",
    "\n",
    "cypher_llm = OllamaLLM(model=\"llama3.1:8b\",\n",
    "                system_prompt=SYSTEM_PROMPT_RAG_EN,\n",
    "                base_url=f\"http://{host}:{setting.ollama.port}\",\n",
    "                temperature=setting.ollama.temperature,\n",
    "                context_window=setting.ollama.context_window,\n",
    "                request_timeout=setting.ollama.request_timeout,\n",
    ")\n",
    "\n",
    "qa_llm = OllamaLLM(model=\"llama3.1:8b\",\n",
    "                # system_prompt=SYSTEM_PROMPT_RAG_EN,\n",
    "                base_url=f\"http://{host}:{setting.ollama.port}\",\n",
    "                temperature=setting.ollama.temperature,\n",
    "                context_window=setting.ollama.context_window,\n",
    "                request_timeout=setting.ollama.request_timeout,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize graph and LLM\n",
    "graph = Neo4jGraph(url=f\"bolt://{host}:7687\", username=\"neo4j\", password=\"password\", database=\"neo4j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama response:  Hello! How are you today? Is there something I can help you with or would you like to chat?\n",
      "Graph response:  [{'n': {'tagline': 'Welcome to the Real World', 'title': 'The Matrix', 'released': 1999}}]\n"
     ]
    }
   ],
   "source": [
    "if check_components_are_running:\n",
    "    print(\"Ollama response: \", cypher_llm.invoke(\"hello\"))\n",
    "    print(\"Graph response: \", graph.query(\"MATCH (n) RETURN n LIMIT 1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chain itself\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    qa_llm=qa_llm,\n",
    "    cypher_llm=cypher_llm,\n",
    "    graph=graph,\n",
    "    allow_dangerous_requests=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Movie) WHERE m.released = 1999 RETURN m\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'m': {'tagline': 'Welcome to the Real World', 'title': 'The Matrix', 'released': 1999}}, {'m': {'tagline': 'First loves last. Forever.', 'title': 'Snow Falling on Cedars', 'released': 1999}}, {'m': {'tagline': \"Walk a mile you'll never forget.\", 'title': 'The Green Mile', 'released': 1999}}, {'m': {'tagline': \"One robot's 200 year journey to become an ordinary man.\", 'title': 'Bicentennial Man', 'released': 1999}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response from the LLM: \n",
      "The Matrix, Snow Falling on Cedars, The Green Mile, Bicentennial Man was released in the year 1999.\n"
     ]
    }
   ],
   "source": [
    "# Query the system\n",
    "response = chain.run(\"What movies were released in the year 1999?\")\n",
    "print(\"Response from the LLM: \")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
